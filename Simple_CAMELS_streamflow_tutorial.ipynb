{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mn5hk/2020a_SSH_mapping_NATL60/blob/master/Simple_CAMELS_streamflow_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd1ZirV733Bn"
      },
      "source": [
        "# MHPI hydroDL Tutorial\n",
        "\n",
        "This code contains deep learning code used to model hydrologic systems from soil moisture to streamflow or from projection to forecast. If this work is useful to you, please cite\n",
        "\n",
        "\n",
        "*   Feng, Dapeng, Kuai Fang and Chaopeng Shen, Enhancing streamflow forecast and extracting insights using continental-scale long-short term memory networks with data integration, Water Resources Research, doi: 10.1029/2019WR026793\n",
        "*   Fang, Kuai and Chaopeng Shen, Near-real-time forecast of satellite-based soil moisture using long short-term memory with an adaptive data integration kernel, Journal of Hydrometeorology, JHM-D-19-0169.1, doi:10.1175/JHM-D-19-0169.1 (2020)\n",
        "\n",
        "\n",
        "\n",
        "[![PyPI](https://img.shields.io/badge/pypi-version%200.1-blue)](https://pypi.org/project/hydroDL/0.1.0/)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3993880.svg)](https://doi.org/10.5281/zenodo.3993880) [![CodeStyle](https://img.shields.io/badge/code%20style-Black-black)]()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQs9j0Xp4NsW"
      },
      "source": [
        "Welcome to our hydroDL tutorial at The Pennsylvania State University! The following notebook is designed to provide a quick start to our project and get you ready to write your own neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSikU4JW4R-X"
      },
      "source": [
        "### Obtain hydroDL from Github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkszvjkQ4VhY",
        "outputId": "113fb853-19ee-41b7-8b1b-564c5aafd545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hydroDLpack'...\n",
            "remote: Enumerating objects: 1165, done.\u001b[K\n",
            "remote: Counting objects: 100% (205/205), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 1165 (delta 116), reused 143 (delta 85), pack-reused 960\u001b[K\n",
            "Receiving objects: 100% (1165/1165), 60.71 MiB | 24.98 MiB/s, done.\n",
            "Resolving deltas: 100% (499/499), done.\n",
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir(\"/content/\")\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "!rm -rf hydroDLpack # removes any old versions of hydroDL (in case you ran this tutorial previously)\n",
        "!git clone https://github.com/mhpi/hydroDL.git hydroDLpack\n",
        "os.chdir(\"/content/hydroDLpack\")\n",
        "!conda develop ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P031zCOp42ZT"
      },
      "source": [
        "### Define Data Download Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGViqzC__BCw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9975763d-2f46-4977-d6d8-f2d4152c1e4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading package hydroDL\n"
          ]
        }
      ],
      "source": [
        "# This block defines functions used to obtain the data used for training and validation of the model. Real download action is in the next block.\n",
        "# If you want to use the code with your own data, you can skip this block.\n",
        "# Because CAMELS data is large and we cannot redistribute it, we provide two options for getting the data\n",
        "# (1) set ReDownloadCAMEL=True in the next block and run that block to download it directly via colab. This can be slow and sometimes, you get cut off by NCAR servers\n",
        "# (2) set ReDownloadCAMEL=False. get a script (download_CAMELS_script.py with path below) to run locally, define training/validation periods and process them into pickle files.\n",
        "#      Upload the pickle files into the colab session to continue.\n",
        "#2 is much faster and more reliable, but requires you have a local python installation with some dependencies.\n",
        "\n",
        "from hydroDL.master import default\n",
        "from hydroDL.data import camels\n",
        "import platform\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "def downloadCAMELS():\n",
        "  if platform.system() == 'Windows':\n",
        "    def download_file(url, destination):\n",
        "        \"\"\"Download a file from a specified URL to a given destination.\"\"\"\n",
        "        response = requests.get(url, stream=True)\n",
        "        with open(destination, 'wb') as file:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                file.write(chunk)\n",
        "\n",
        "    def unzip_file(source, destination):\n",
        "        \"\"\"Unzip a file from a source to a destination.\"\"\"\n",
        "        with zipfile.ZipFile(source, 'r') as zip_ref:\n",
        "            zip_ref.extractall(destination)\n",
        "\n",
        "    # Base directory\n",
        "    base_dir = os.getcwd()\n",
        "\n",
        "    # Create necessary directories\n",
        "    os.makedirs(base_dir + 'camels_attributes_v2.0/', exist_ok=True)\n",
        "    os.makedirs(base_dir + 'camels_attributes_v2.0/camels_attributes_v2.0/', exist_ok=True)\n",
        "\n",
        "    # Download and unzip the main dataset\n",
        "    main_dataset_url = 'https://gdex.ucar.edu/dataset/camels/file/basin_timeseries_v1p2_metForcing_obsFlow.zip'\n",
        "    main_dataset_dest = base_dir + 'basin_timeseries_v1p2_metForcing_obsFlow.zip'\n",
        "    download_file(main_dataset_url, main_dataset_dest)\n",
        "    unzip_file(main_dataset_dest, base_dir + 'basin_timeseries_v1p2_metForcing_obsFlow/')\n",
        "\n",
        "    # List of other files to download\n",
        "    files_to_download = {\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.xlsx': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_attributes_v2.0.xlsx',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_clim.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_clim.txt',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_geol.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_geol.txt',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_hydro.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_hydro.txt',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_name.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_name.txt',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_soil.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_soil.txt',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_topo.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_topo.txt',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_vege.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_vege.txt'\n",
        "    }\n",
        "\n",
        "    # Download additional files\n",
        "    for url, dest in files_to_download.items():\n",
        "        full_dest = os.path.join(base_dir, dest)\n",
        "        download_file(url, full_dest)\n",
        "\n",
        "    print(\"Download and extraction complete.\")\n",
        "\n",
        "# Add additional download and unzip commands as needed\n",
        "\n",
        "  else:\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/basin_timeseries_v1p2_metForcing_obsFlow.zip' -O '/content/basin_timeseries_v1p2_metForcing_obsFlow.zip'\n",
        "    !unzip '/content/basin_timeseries_v1p2_metForcing_obsFlow.zip' -d '/content/basin_timeseries_v1p2_metForcing_obsFlow'\n",
        "    !mkdir '/content/camels_attributes_v2.0/'\n",
        "    !mkdir '/content/camels_attributes_v2.0/camels_attributes_v2.0/'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.xlsx' -O '/content/camels_attributes_v2.0/camels_attributes_v2.0/camels_attributes_v2.0.xlsx'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_clim.txt' -O '/content/camels_attributes_v2.0/camels_attributes_v2.0/camels_clim.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_geol.txt' -O '/content/camels_attributes_v2.0/camels_attributes_v2.0/camels_geol.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_hydro.txt' -O '/content/camels_attributes_v2.0/camels_attributes_v2.0/camels_hydro.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_name.txt' -O '/content/camels_attributes_v2.0/camels_attributes_v2.0/camels_name.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_soil.txt' -O '/content/camels_attributes_v2.0/camels_attributes_v2.0/camels_soil.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_topo.txt' -O '/content/camels_attributes_v2.0/camels_attributes_v2.0/camels_topo.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_soil.txt' -O '/content/camels_attributes_v2.0/camels_attributes_v2.0/camels_soil.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_vege.txt' -O '/content/camels_attributes_v2.0/camels_attributes_v2.0/camels_vege.txt'\n",
        "\n",
        "\n",
        "def extractCAMELS(Ttrain,attrLst,varF,camels,forType='daymet',flow_regime=1,subset_train=\"All\",file_path=None):\n",
        "  # flow_regime==1: high flow expert procedures\n",
        "  train_loader = camels.DataframeCamels(subset=subset_train, tRange=Ttrain, forType=forType)\n",
        "  x = train_loader.getDataTs(varLst=varF, doNorm=False, rmNan=False, flow_regime=flow_regime)\n",
        "  y = train_loader.getDataObs(doNorm=False, rmNan=False, basinnorm=False, flow_regime=flow_regime)\n",
        "  c = train_loader.getDataConst(varLst=attrLst, doNorm=False, rmNan=False, flow_regime=flow_regime)\n",
        "  # define dataset\n",
        "  if file_path is not None:\n",
        "    with open(file_path, 'wb') as f:\n",
        "      pickle.dump((x, y, c), f)\n",
        "  return x, y, c"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Data (READ! Select how data is obtained)"
      ],
      "metadata": {
        "id": "k0ue93ohidPm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKByQB-7lMQZ",
        "outputId": "61e91729-9724-4193-b060-eeca4b096fc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function hydroDL.data.camels.initcamels(flow_regime, forType, rootDB='/scratch/Camels')>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# If you want to re-download CAMELS from scratch, set ReDownloadCAMEL=True and run this cell\n",
        "# WARNING: Due to NCAR server behaviors, LONG run-time may be needed (and if obtaining errors when loading dataset later, may need to check to ensure everything downloaded correctly/run again)\n",
        "# A faster and more reliable approach is to get this code and run it locally (brief instructions on how to run the script are included):\n",
        "# https://github.com/mhpi/hydroDL/blob/release/example/download_CAMELS_script.py\n",
        "# This script defines training/validation time periods and variables etc\n",
        "# You will get training_file and validation_file after running the above script. Upload them under hydroDLpack in the Colab session\n",
        "ReDownloadCAMEL=False\n",
        "if ReDownloadCAMEL:\n",
        "  downloadCAMELS() #KL: should this be commented out by default then, if we're going to load from pickle file by default?\n",
        "  justLoadXYC = False #KL: make true if using pickle files, false if downloading data directly\n",
        "else:\n",
        "  # Manually upload training_file and validation_file under hydroDLpack before proceeding!\n",
        "  justLoadXYC = True #KL: make true if using pickle files, false if downloading data directly\n",
        "  #KL: If you are starting with pickle files (or files you downloaded manually or using a script) instead of directly downloading the CAMELS data from the source:\n",
        "  #KL: Click on the folder icon on the left-hand side, then select the option for file upload, and choose both pickle files. This seems to run faster than the alternative below.\n",
        "\n",
        "  #KL: Alternatively, run the following lines of code and when prompted (\"browse\" button will show below), select the files to upload\n",
        "  #KL: (if pickle files, will be named \"training_file\" and \"validation_file\"). Took us ~30 minutes.\n",
        "  #from google.colab import files\n",
        "  #uploaded = files.upload()\n",
        "camels.initcamels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1n3TnEDYOZ_"
      },
      "source": [
        "### Define Statistics Functions and a Scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYNumHtL1flL"
      },
      "outputs": [],
      "source": [
        "# This part defines a hydrology-specific Scaler class that works similar as sklearn.MinMaxScaler\n",
        "# getStatDic, calcStat, calcStatgamma are supporting functions\n",
        "# If you want to use it with your own data, you can create your own scaler that can be used to normalize your data\n",
        "# Later on, we will use this scaler to transform the train and val data\n",
        "\n",
        "def getStatDic(log_norm_cols, attrLst=None, attrdata=None, seriesLst=None, seriesdata=None):\n",
        "  statDict = dict()\n",
        "  # series data\n",
        "  if seriesLst is not None:\n",
        "    for k in range(len(seriesLst)):\n",
        "      var = seriesLst[k]\n",
        "      if var in log_norm_cols:\n",
        "        statDict[var] = calcStatgamma(seriesdata[:, :, k])\n",
        "      else:\n",
        "        statDict[var] = calcStat(seriesdata[:, :, k])\n",
        "\n",
        "  # const attribute\n",
        "  if attrLst is not None:\n",
        "    for k in range(len(attrLst)):\n",
        "      var = attrLst[k]\n",
        "      statDict[var] = calcStat(attrdata[:, k])\n",
        "  return statDict\n",
        "\n",
        "def calcStat(x):\n",
        "  a = x.flatten()\n",
        "  b = a[~np.isnan(a)]\n",
        "  p10 = np.percentile(b, 10).astype(float)\n",
        "  p90 = np.percentile(b, 90).astype(float)\n",
        "  mean = np.mean(b).astype(float)\n",
        "  std = np.std(b).astype(float)\n",
        "  if std < 0.001:\n",
        "    std = 1\n",
        "  return [p10, p90, mean, std]\n",
        "\n",
        "def calcStatgamma(x):  # for daily streamflow and precipitation\n",
        "  a = x.flatten()\n",
        "  b = a[~np.isnan(a)]  # kick out Nan\n",
        "  b = np.log10(\n",
        "    np.sqrt(b) + 0.1\n",
        "  )  # do some tranformation to change gamma characteristics\n",
        "  p10 = np.percentile(b, 10).astype(float)\n",
        "  p90 = np.percentile(b, 90).astype(float)\n",
        "  mean = np.mean(b).astype(float)\n",
        "  std = np.std(b).astype(float)\n",
        "  if std < 0.001:\n",
        "    std = 1\n",
        "  return [p10, p90, mean, std]\n",
        "\n",
        "def transNormbyDic( x_in, var_lst, stat_dict, log_norm_cols, to_norm):\n",
        "  if type(var_lst) is str:\n",
        "    var_lst = [var_lst]\n",
        "  x = x_in.copy()\n",
        "  out = np.full(x.shape, np.nan)\n",
        "  for k in range(len(var_lst)):\n",
        "    var = var_lst[k]\n",
        "    stat = stat_dict[var]\n",
        "    if to_norm is True:\n",
        "      if len(x.shape) == 3:\n",
        "        if var in log_norm_cols:\n",
        "          x[:, :, k] = np.log10(np.sqrt(x[:, :, k]) + 0.1)\n",
        "        out[:, :, k] = (x[:, :, k] - stat[2]) / stat[3]\n",
        "      elif len(x.shape) == 2:\n",
        "        if var in log_norm_cols:\n",
        "          x[:, k] = np.log10(np.sqrt(x[:, k]) + 0.1)\n",
        "        out[:, k] = (x[:, k] - stat[2]) / stat[3]\n",
        "    else:\n",
        "      if len(x.shape) == 3:\n",
        "        out[:, :, k] = x[:, :, k] * stat[3] + stat[2]\n",
        "        if var in log_norm_cols:\n",
        "          out[:, :, k] = (np.power(10, out[:, :, k]) - 0.1) ** 2\n",
        "      elif len(x.shape) == 2:\n",
        "        out[:, k] = x[:, k] * stat[3] + stat[2]\n",
        "        if var in log_norm_cols:\n",
        "          out[:, k] = (np.power(10, out[:, k]) - 0.1) ** 2\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "class HydroScaler:\n",
        "  def __init__(self, attrLst, seriesLst, xNanFill,log_norm_cols):\n",
        "    self.log_norm_cols = log_norm_cols\n",
        "    self.attrLst = attrLst\n",
        "    self.seriesLst = seriesLst\n",
        "    self.stat_dict = None\n",
        "    self.xNanFill = xNanFill\n",
        "\n",
        "  def fit(self, attrdata, seriesdata):\n",
        "    self.stat_dict = getStatDic(\n",
        "      log_norm_cols=self.log_norm_cols,\n",
        "      attrLst=self.attrLst,\n",
        "      attrdata=attrdata,\n",
        "      seriesLst=self.seriesLst,\n",
        "      seriesdata=seriesdata,\n",
        "    )\n",
        "\n",
        "  def transform(self, data, var_list,):\n",
        "\n",
        "    norm_data = transNormbyDic(\n",
        "      data, var_list, self.stat_dict, log_norm_cols = self.log_norm_cols, to_norm=True)\n",
        "\n",
        "    return norm_data\n",
        "\n",
        "  def fit_transform(self, attrdata, seriesdata):\n",
        "    self.fit(attrdata, seriesdata)\n",
        "    attr_norm = self.transform(attrdata, self.attrLst)\n",
        "    series_norm = self.transform(seriesdata, self.seriesLst)\n",
        "    return attr_norm, series_norm\n",
        "\n",
        "\n",
        "# The function it only used for streamflow unit convert and nondimensionalization\n",
        "def basinNorm(x, basinarea, meanprep, toNorm):\n",
        "  nd = len(x.shape)\n",
        "  if nd == 3 and x.shape[2] == 1:\n",
        "    x = x[:, :, 0]  # unsqueeze the original 3 dimension matrix\n",
        "  temparea = np.tile(basinarea, (1, x.shape[1]))\n",
        "  tempprep = np.tile(meanprep, (1, x.shape[1]))\n",
        "  if toNorm is True:\n",
        "    flow = (x * 0.0283168 * 3600 * 24) / (\n",
        "            (temparea * (10 ** 6)) * (tempprep * 10 ** (-3))\n",
        "    )  # (m^3/day)/(m^3/day)\n",
        "  else:\n",
        "\n",
        "    flow = (\n",
        "            x\n",
        "            * ((temparea * (10 ** 6)) * (tempprep * 10 ** (-3)))\n",
        "            / (0.0283168 * 3600 * 24)\n",
        "    )\n",
        "  if nd == 3:\n",
        "    flow = np.expand_dims(flow, axis=2)\n",
        "  return flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmX3xZ8BYYPX"
      },
      "source": [
        "### Initialize hydroDL Library, Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQD2T9TUJ072"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import os\n",
        "from tqdm import trange\n",
        "import random\n",
        "import hydroDL.core.logger as logger\n",
        "from hydroDL.master.master import loadModel, wrapMaster, writeMasterFile\n",
        "from hydroDL.model.rnn import CudnnLstmModel, CpuLstmModel\n",
        "from hydroDL.model.crit import RmseLoss, NSELossBatch,NSESqrtLossBatch\n",
        "from hydroDL.master import default\n",
        "from hydroDL.data import camels\n",
        "import numpy as np\n",
        "import json\n",
        "log = logger.get_logger(\"model.train_.train\")\n",
        "\n",
        "rootDatabase = \"/content/\"\n",
        "device = torch.cuda.current_device() if torch.cuda.is_available() else torch.device(\"cpu\" )\n",
        "\n",
        "\n",
        "# Fix random seed\n",
        "seedid = 111111\n",
        "random.seed(seedid)\n",
        "torch.manual_seed(seedid)\n",
        "np.random.seed(seedid)\n",
        "torch.cuda.manual_seed(seedid)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set hyperparameters\n",
        "EPOCH = 20 #KL: Reduced from 300 to 20 to save time/computational power when testing\n",
        "BATCH_SIZE = 100\n",
        "RHO = 365\n",
        "HIDDENSIZE = 256\n",
        "saveEPOCH = 20  # save model for every \"saveEPOCH\" epochs\n",
        "trainBuff = 365\n",
        "loadTrain = True\n",
        "subset_train = 'All'  # give the list of basins to train on or else fix 'All' to use all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gizlgXKOYhyR"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz-Z1nQEKug3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Dataset-specific code to read and scale the data\n",
        "IF YOU CAN DIRECTLY PROVIDE DATA for train_x, train_y, train_c, val_x, val_y, val_c and scale the data\n",
        "you can skip this section\n",
        "The data structure is as follows:\n",
        "train_x (forcing data, e.g. precipitation, temperature ...), shape: [pixels, time, features]\n",
        "train_c (constant data, e.g. soil properties, land cover ...), shape: [pixels, features]\n",
        "target/train_y (e.g. soil moisture, streamflow ...), shape: [pixels, time, 1]\n",
        "val_x, val_c, val_y\n",
        "Data type: numpy.float\n",
        "The Scaler does\n",
        "attr_norm, series_norm = scaler.fit_transform(train_c, series_data)\n",
        "val_c = scaler.transform(val_c, attrLst)\n",
        "val_x = scaler.transform(val_x, varF)\n",
        "\"\"\"\n",
        "\n",
        "import pickle\n",
        "\n",
        "# These variables below define some options\n",
        "# If you used download_CAMELS_script.py as described above. Some of these options are repeated in there\n",
        "# However, they are put here because\n",
        "forType = 'daymet'\n",
        "flow_regime = 1\n",
        "Ttrain = [19801001, 19951001] #training period\n",
        "valid_date = [19951001, 20101001]  # Testing period\n",
        "#define inputs\n",
        "if forType == 'daymet':\n",
        "  varF = ['dayl', 'prcp', 'srad', 'tmean', 'vp']\n",
        "else:\n",
        "  varF = ['dayl', 'prcp', 'srad', 'tmax', 'vp']\n",
        "\n",
        "train_file = 'training_file'\n",
        "validation_file = 'validation_file'\n",
        "# Define attributes list\n",
        "attrLst = [ 'p_mean','pet_mean', 'p_seasonality', 'frac_snow', 'aridity', 'high_prec_freq', 'high_prec_dur',\n",
        "            'low_prec_freq', 'low_prec_dur', 'elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max',\n",
        "            'lai_diff', 'gvf_max', 'gvf_diff', 'dom_land_cover_frac', 'dom_land_cover', 'root_depth_50',\n",
        "            'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity',\n",
        "            'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'geol_1st_class', 'glim_1st_class_frac',\n",
        "            'geol_2nd_class', 'glim_2nd_class_frac', 'carbonate_rocks_frac', 'geol_porostiy', 'geol_permeability']\n",
        "\n",
        "opt_data = default.optDataCamels # wrapping options around.\n",
        "opt_data = default.update(opt_data, varT=varF, varC=attrLst, tRange=Ttrain, forType=forType, subset = subset_train)\n",
        "\n",
        "if justLoadXYC:\n",
        "    # Load X, Y, C from a file\n",
        "    with open(train_file, 'rb') as f:\n",
        "        train_x, train_y, train_c = pickle.load(f)  # Adjust this line based on your file format\n",
        "    with open(validation_file, 'rb') as g:\n",
        "        val_x, val_y, val_c = pickle.load(g)  # Adjust this line based on your file format\n",
        "else:\n",
        "  # a CAMELS dataset-specific object to manage basin normalization\n",
        "  # If you are not using CAMELS, you can discard this one\n",
        "  camels.initcamels(flow_regime=flow_regime, forType=forType, rootDB=rootDatabase)\n",
        "  train_x, train_y, train_c = extractCAMELS(Ttrain,attrLst,varF,camels,forType='daymet',flow_regime=1,subset_train=\"All\",file_path=train_file)\n",
        "  val_x, val_y, val_c = extractCAMELS(valid_date,attrLst,varF,camels,forType='daymet',flow_regime=1,subset_train=\"All\",file_path=validation_file)\n",
        "\n",
        "\n",
        "# get a Scaler (similar to sklearn.MinMaxScaler).\n",
        "# basinNorm function is only for converting the streamflow unit from ft^3/s to dimensionless.  The basin area and annual mean of precipiataion are required. y_temp = train_y/(basinarea*meanprep)\n",
        "# If your streamflow is not in the unit of ft^3/s, this function needs to be adapted\n",
        "basinarea  = val_c[:,np.where(np.array(attrLst)=='area_gages2')[0]]\n",
        "meanprep  = val_c[:,np.where(np.array(attrLst)=='p_mean')[0]]\n",
        "##Dimensionless streamflow\n",
        "y_temp = basinNorm(train_y,  basinarea, meanprep, toNorm=True)\n",
        "\n",
        "#train_x[np.isnan(train_x)] = 0.0 #KL: what is this? provide comment/context for when this should be uncommented\n",
        "series_data = np.concatenate([train_x, y_temp], axis=2)\n",
        "series_var_list = varF + [\"runoff\"]\n",
        "\n",
        "# Initialize scaler\n",
        "# This scaler will be used to scale the training data and later for the test as well.\n",
        "# If flow_regime = 0, we will do a log transform and standard normalization on precipitation and streamflow; the other variables only use the standard normalization.\n",
        "# If flow_regime = 1, we will only do standard normalization on all variables.\n",
        "if flow_regime == 0:\n",
        "    log_norm_cols = [ \"prcp\", \"usgsFlow\", \"Precip\", \"runoff\",  \"Runoff\", \"Runofferror\" ]\n",
        "else:\n",
        "    log_norm_cols = []\n",
        "\n",
        "#train_c[np.isnan(train_c)] = 0.0 #KL: what is this? provide comment/context for when this should be uncommented\n",
        "# Calculate the statistics for scaling the training data; Same statistics should be used for validation data for consistency. D\n",
        "scaler = HydroScaler(attrLst=attrLst, seriesLst=series_var_list, xNanFill=0.0, log_norm_cols=log_norm_cols)\n",
        "# Fit and transform training data\n",
        "\n",
        "\n",
        "attr_norm, series_norm = scaler.fit_transform(train_c, series_data)\n",
        "attr_norm[np.isnan(attr_norm)] = 0.0\n",
        "\n",
        "train_x = series_norm[:, :, :-1]  # Forcing data\n",
        "train_x[np.isnan(train_x)] = 0.0\n",
        "train_y = np.expand_dims(series_norm[:, :, -1], 2)\n",
        "train_c = attr_norm\n",
        "\n",
        "# Fit and transform validation data\n",
        "\n",
        "val_c = scaler.transform(val_c, attrLst)\n",
        "val_x = scaler.transform(val_x, varF)\n",
        "# no need to scale val_y before we transform the prediction back to compare with val_y\n",
        "\n",
        "# fill some gaps in x. A fill for the normalized data is equal to filling with mean values\n",
        "# We don't fill y because it will be handled by the loss function\n",
        "val_c[np.isnan(val_c)] = 0.0\n",
        "val_x[np.isnan(val_x)] = 0.0\n",
        "\n",
        "#IF YOU CAN DIRECTLY PROVIDE DATA for train_x, train_y, train_c, val_x, val_y, val_c, and a Scaler and scale the data\n",
        "# you can skip this section\n",
        "#later code used scaler.stat_dict that matches with this scaler for testing. That could be changed for your data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_x))\n",
        "print(train_x.shape, train_y.shape, train_c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6Lz2vy1VZWN",
        "outputId": "f9e6667c-02b6-45a8-84cb-5dfaf2f8fdd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(671, 5478, 5) (671, 5478, 1) (671, 35)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQCCtsxSYklB"
      },
      "source": [
        "### Intialize some options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlmubU0zLim9"
      },
      "outputs": [],
      "source": [
        "# define model and update configure\n",
        "if torch.cuda.is_available():\n",
        "    opt_model = default.optLstm\n",
        "else:\n",
        "    opt_model = default.update(default.optLstm, name=\"hydroDL.model.rnn.CpuLstmModel\")\n",
        "opt_model = default.update(default.optLstm, hiddenSize=HIDDENSIZE)\n",
        "\n",
        "# this goes with your choice of flow_regime\n",
        "if flow_regime==0:\n",
        "    optLoss = default.optLossRMSE\n",
        "elif flow_regime==1:\n",
        "    optLoss = default.optLossNSEBatch\n",
        "\n",
        "opt_train = default.update(default.optTrainCamels, miniBatch=[BATCH_SIZE, RHO], nEpoch=EPOCH, saveEpoch=1, seed=seedid, trainBuff=trainBuff)\n",
        "\n",
        "# define output folder for model results\n",
        "exp_name = f\"CAMELSDemo\"\n",
        "exp_disp = \"TestRun\"\n",
        "save_path = os.path.join(\n",
        "    exp_name,\n",
        "    exp_disp,\n",
        "    \"epochs{}_batch{}_rho{}_hiddensize{}_Tstart{}_Tend{}_trainBuff{}_flowregime{}\".format(\n",
        "        opt_train[\"nEpoch\"],\n",
        "        opt_train[\"miniBatch\"][0],\n",
        "        opt_train[\"miniBatch\"][1],\n",
        "        opt_model[\"hiddenSize\"],\n",
        "        opt_data[\"tRange\"][0],\n",
        "        opt_data[\"tRange\"][1],\n",
        "        opt_train['trainBuff'],\n",
        "        flow_regime,\n",
        "    ),\n",
        ")\n",
        "rootOut = os.path.join(\n",
        "    os.sep, \"content\", \"output\", \"rnnStreamflow\"\n",
        ")\n",
        "output_path = os.path.join(rootOut, save_path, \"All\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "YREo_Tn2LA4C",
        "outputId": "687ccb4c-c20e-4e11-bf85-88d877b87b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "write master file /content/output/rnnStreamflow/CAMELSDemo/TestRun/epochs20_batch100_rho365_hiddensize256_Tstart19801001_Tend19951001_trainBuff365_flowregime1/All/master.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/output/rnnStreamflow/CAMELSDemo/TestRun/epochs20_batch100_rho365_hiddensize256_Tstart19801001_Tend19951001_trainBuff365_flowregime1/All'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# define training options\n",
        "\n",
        "nx = train_x.shape[-1] + train_c.shape[-1]  # update nx, nx = nx + nc\n",
        "ny = train_y.shape[-1]\n",
        "#load model for training\n",
        "if torch.cuda.is_available():\n",
        "    model = CudnnLstmModel(nx=nx, ny=ny, hiddenSize=HIDDENSIZE).to(device)\n",
        "else:\n",
        "    model = CpuLstmModel(nx=nx, ny=ny, hiddenSize=HIDDENSIZE).to(device)\n",
        "\n",
        "opt_model = default.update(opt_model, nx=nx, ny=ny)\n",
        "if flow_regime==0:\n",
        "  lossFun = RmseLoss()\n",
        "elif flow_regime==1:\n",
        "  lossFun = NSELossBatch(np.nanstd(train_y, axis=1),device=device)\n",
        "\n",
        "# update and write the dictionary variable to out folder for logging and future testing\n",
        "opt_all = wrapMaster(output_path, opt_data, opt_model, optLoss, opt_train)\n",
        "writeMasterFile(opt_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjI9gigXw8bj"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Dataloader functions for the training function. These functions are typically embedded inside hydroDL package,\n",
        "but they are pulled out here so that newcomers can understand them without packaging details.\n",
        "They help to build the minibatch and copy to cuda() device\n",
        "Here we extract a minibatch from inputs[\"x\", \"c\", \"y\"] and wrap the extracted data to data[\"x\", \"yTrain\"]\n",
        "'''\n",
        "\n",
        "def select_subset(\n",
        "    x, iGrid, iT, rho, c=None, tupleOut=False, LCopt=False, bufftime=0, **kwargs\n",
        "):\n",
        "    \"\"\"\n",
        "    Selects a subset based on the grid given\n",
        "    :param x:\n",
        "    :param iGrid:\n",
        "    :param iT:\n",
        "    :param rho:\n",
        "    :param c:\n",
        "    :param tupleOut:\n",
        "    :param LCopt:\n",
        "    :param bufftime:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    nx = x.shape[-1]\n",
        "    nt = x.shape[1]\n",
        "    if x.shape[0] == len(iGrid):  # hack\n",
        "        iGrid = np.arange(0, len(iGrid))  # hack\n",
        "    if nt <= rho:\n",
        "        iT.fill(0)\n",
        "    batchSize = iGrid.shape[0]\n",
        "    if iT is not None:\n",
        "        # batchSize = iGrid.shape[0]\n",
        "        xTensor = torch.zeros([rho + bufftime, batchSize, nx], requires_grad=False)\n",
        "        for k in range(batchSize):\n",
        "            temp = x[\n",
        "                iGrid[k] : iGrid[k] + 1, np.arange(iT[k] - bufftime, iT[k] + rho), :\n",
        "            ]\n",
        "            xTensor[:, k : k + 1, :] = torch.from_numpy(np.swapaxes(temp, 1, 0))\n",
        "    else:\n",
        "        if LCopt is True:\n",
        "            # used for local calibration kernel: FDC, SMAP...\n",
        "            if len(x.shape) == 2:\n",
        "                # Used for local calibration kernel as FDC\n",
        "                # x = Ngrid * Ntime\n",
        "                xTensor = torch.from_numpy(x[iGrid, :]).float()\n",
        "            elif len(x.shape) == 3:\n",
        "                # used for LC-SMAP x=Ngrid*Ntime*Nvar\n",
        "                xTensor = torch.from_numpy(np.swapaxes(x[iGrid, :, :], 1, 2)).float()\n",
        "        else:\n",
        "            # Used for rho equal to the whole length of time series\n",
        "            xTensor = torch.from_numpy(np.swapaxes(x[iGrid, :, :], 1, 0)).float()\n",
        "            rho = xTensor.shape[0]\n",
        "    if c is not None:\n",
        "        nc = c.shape[-1]\n",
        "        temp = np.repeat(\n",
        "            np.reshape(c[iGrid, :], [batchSize, 1, nc]), rho + bufftime, axis=1\n",
        "        )\n",
        "        cTensor = torch.from_numpy(np.swapaxes(temp, 1, 0)).float()\n",
        "        if tupleOut:\n",
        "            if torch.cuda.is_available():\n",
        "                xTensor = xTensor.cuda()\n",
        "                cTensor = cTensor.cuda()\n",
        "            out = (xTensor, cTensor)\n",
        "        else:\n",
        "            out = torch.cat((xTensor, cTensor), 2)\n",
        "    else:\n",
        "        out = xTensor\n",
        "    if torch.cuda.is_available() and type(out) is not tuple:\n",
        "        out = out.cuda()\n",
        "    return out\n",
        "\n",
        "\n",
        "def random_index(ngrid, nt, dimSubset, bufftime=0):\n",
        "    \"\"\"\n",
        "    Finds a random place to start inside of the grids\n",
        "    :param: ngrid: the number of grids\n",
        "    :param: nt: the number of tiles\n",
        "    :param: dimSubset: the dimensions of the subset\n",
        "    :param: bufftime: a buffer\n",
        "    :returns: the index of the grid and the index of the tile\n",
        "    \"\"\"\n",
        "    batchSize, rho = dimSubset\n",
        "    iGrid = np.random.randint(0, ngrid, [batchSize])\n",
        "    iT = np.random.randint(0 + bufftime, nt - rho, [batchSize])\n",
        "    return iGrid, iT\n",
        "\n",
        "def save_model(outFolder, model, epoch, modelName=\"model\"):\n",
        "    modelFile = os.path.join(outFolder, modelName + \"_Ep\" + str(epoch) + \".pt\")\n",
        "    torch.save(model, modelFile)\n",
        "\n",
        "\n",
        "def load_model(outFolder, epoch, modelName=\"model\"):\n",
        "    modelFile = os.path.join(outFolder, modelName + \"_Ep\" + str(epoch) + \".pt\")\n",
        "    model = torch.load(modelFile)\n",
        "    return model\n",
        "\n",
        "def load_data(inputs, settings):\n",
        "    \"\"\"\n",
        "\n",
        "    :param inputs:\n",
        "    :param settings:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "    iGrid, iT = random_index(\n",
        "        settings[\"ngrid\"], settings[\"nt\"], [settings[\"batchSize\"], settings[\"rho\"]], bufftime=settings['bufftime']\n",
        "    )\n",
        "    data[\"x\"] = select_subset(inputs[\"x\"], iGrid, iT, settings[\"rho\"], c=inputs[\"c\"], bufftime=settings['bufftime'])\n",
        "    data[\"yTrain\"] = select_subset(inputs[\"y\"], iGrid, iT, settings[\"rho\"])\n",
        "    return data, iGrid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbQgKEzYYuMx"
      },
      "source": [
        "### Define Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfUOTMMNKDDx"
      },
      "outputs": [],
      "source": [
        "from hydroDL.model.settings import make_train_settings\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from tqdm import trange\n",
        "import hydroDL.core.logger as logger\n",
        "from hydroDL.model import crit\n",
        "\n",
        "def trainModel(\n",
        "    model,\n",
        "    x,\n",
        "    y,\n",
        "    c,\n",
        "    lossFun,\n",
        "    nEpoch=500,\n",
        "    load_data=load_data,\n",
        "    miniBatch=[100, 30],\n",
        "    saveEpoch=100,\n",
        "    saveFolder=None,\n",
        "    mode=\"seq2seq\",\n",
        "    bufftime=0,\n",
        "):\n",
        "    optim = torch.optim.Adadelta(model.parameters())\n",
        "    if saveFolder is not None:\n",
        "        runFile = os.path.join(saveFolder, \"run.csv\")\n",
        "        rf = open(runFile, \"w+\")\n",
        "    inputs, settings = make_train_settings(model, x, y, c, nEpoch, miniBatch, bufftime,)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        lossFun = lossFun.cuda()\n",
        "\n",
        "    #_model_ = ModelWrapper(model)  # wraps up so the model can accept a dict\n",
        "    # for illustration in this notebook, this wrapping is disabled.\n",
        "    with trange(1, nEpoch + 1) as pbar:\n",
        "        for iEpoch in pbar:\n",
        "            pbar.set_description(f\"Training {model.name}\")\n",
        "            lossEp = 0\n",
        "            t0 = time.time()\n",
        "            for iIter in range(0, settings[\"nIterEp\"]):\n",
        "                model.zero_grad()\n",
        "                dataDict, iGrid = load_data(inputs, settings)\n",
        "                x = dataDict[\"x\"]\n",
        "                yp = model(x)\n",
        "\n",
        "                if type(lossFun) in [NSELossBatch, NSESqrtLossBatch]:\n",
        "                    loss = lossFun(yp[bufftime:, :, :], dataDict[\"yTrain\"], igrid=iGrid)\n",
        "                else:\n",
        "                    loss = lossFun(yp[bufftime:, :, :], dataDict[\"yTrain\"])\n",
        "                    # Additional handling code if needed\n",
        "\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                lossEp = lossEp + loss.item()\n",
        "                # if iIter % 1 == 0:\n",
        "                #     print('Iter {} of {}: Loss {:.3f}'.format(iIter, settings[\"nIterEp\"], loss.item()))\n",
        "            lossEp = lossEp / settings[\"nIterEp\"]\n",
        "            loss_str = \"Epoch {} Loss {:.3f} time {:.2f}\".format(\n",
        "                iEpoch, lossEp, time.time() - t0\n",
        "            )\n",
        "            print(loss_str)\n",
        "            log.debug(loss_str)\n",
        "            pbar.set_postfix(loss=lossEp)\n",
        "            sleep(0.1)\n",
        "            if saveFolder is not None:\n",
        "                rf.write(loss_str + \"\\n\")\n",
        "                if iEpoch % saveEpoch == 0:\n",
        "                    # save model\n",
        "                    modelFile = os.path.join(\n",
        "                        saveFolder, \"model_Ep\" + str(iEpoch) + \".pt\"\n",
        "                    )\n",
        "                    torch.save(model, modelFile)\n",
        "    if saveFolder is not None:\n",
        "        rf.close()\n",
        "\n",
        "    # return model\n",
        "    return model #KL: added to test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7M9sxUVY1aS"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItvUXijZh-Mw",
        "outputId": "62224bfd-aa27-41bd-c6eb-0ac2ebce0ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:   0%|          | 0/20 [00:00<?, ?it/s]/content/hydroDLpack/hydroDL/model/rnn/CudnnLstm.py:105: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)\n",
            "  output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n",
            "Training CudnnLstmModel:   5%|▌         | 1/20 [00:44<13:58, 44.11s/it, loss=0.534]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss 0.534 time 44.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  10%|█         | 2/20 [01:27<13:02, 43.46s/it, loss=0.39]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss 0.390 time 42.90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  15%|█▌        | 3/20 [02:10<12:20, 43.54s/it, loss=0.338]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss 0.338 time 43.53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  20%|██        | 4/20 [02:54<11:39, 43.72s/it, loss=0.311]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss 0.311 time 43.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  25%|██▌       | 5/20 [03:38<10:58, 43.89s/it, loss=0.289]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss 0.289 time 44.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  30%|███       | 6/20 [04:23<10:17, 44.11s/it, loss=0.273]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss 0.273 time 44.43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  35%|███▌      | 7/20 [05:08<09:38, 44.52s/it, loss=0.265]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss 0.265 time 45.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  40%|████      | 8/20 [05:55<09:01, 45.16s/it, loss=0.257]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss 0.257 time 46.42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  45%|████▌     | 9/20 [06:40<08:17, 45.24s/it, loss=0.244]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss 0.244 time 45.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  50%|█████     | 10/20 [07:25<07:30, 45.02s/it, loss=0.239]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Loss 0.239 time 44.45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  55%|█████▌    | 11/20 [08:09<06:43, 44.81s/it, loss=0.238]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Loss 0.238 time 44.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  60%|██████    | 12/20 [08:54<05:57, 44.69s/it, loss=0.23]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Loss 0.230 time 44.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  65%|██████▌   | 13/20 [09:38<05:11, 44.56s/it, loss=0.225]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Loss 0.225 time 44.16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  70%|███████   | 14/20 [10:22<04:27, 44.55s/it, loss=0.219]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Loss 0.219 time 44.42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  75%|███████▌  | 15/20 [11:07<03:42, 44.44s/it, loss=0.215]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Loss 0.215 time 44.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  80%|████████  | 16/20 [11:51<02:57, 44.39s/it, loss=0.212]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Loss 0.212 time 44.18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  85%|████████▌ | 17/20 [12:35<02:13, 44.35s/it, loss=0.21]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Loss 0.210 time 44.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  90%|█████████ | 18/20 [13:19<01:28, 44.37s/it, loss=0.203]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Loss 0.203 time 44.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel:  95%|█████████▌| 19/20 [14:04<00:44, 44.33s/it, loss=0.203]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 Loss 0.203 time 44.13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training CudnnLstmModel: 100%|██████████| 20/20 [14:48<00:00, 44.44s/it, loss=0.2]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 Loss 0.200 time 44.43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# training the model\n",
        "model = trainModel(\n",
        "    model,\n",
        "    train_x,\n",
        "    train_y,\n",
        "    train_c,\n",
        "    lossFun,\n",
        "    nEpoch=EPOCH,\n",
        "    miniBatch=[BATCH_SIZE, RHO],\n",
        "    saveEpoch=saveEPOCH,\n",
        "    saveFolder=output_path,\n",
        "    bufftime=trainBuff\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#need to load model. Modify path if needed, for example with number of epochs used\n",
        "model = torch.load(f'/content/output/rnnStreamflow/CAMELSDemo/TestRun/epochs20_batch100_rho365_hiddensize256_Tstart19801001_Tend19951001_trainBuff365_flowregime1/All/model_Ep{EPOCH}.pt')"
      ],
      "metadata": {
        "id": "usg04258wkI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3ub3mqGY5jK"
      },
      "source": [
        "### Intialize Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_2kfYWuiAJY"
      },
      "outputs": [],
      "source": [
        "from hydroDL.master.master import readMasterFile\n",
        "\n",
        "# validate the result\n",
        "# load validation datasets\n",
        "# load your data. same as training data\n",
        "valid_epoch = EPOCH  # choose the model to test after trained \"valid_epoch\" epoches\n",
        "valid_batch = 50  # do batch forward to save GPU memory\n",
        "\n",
        "# adding some buffer data to validation data\n",
        "testTrainBuff = True\n",
        "loadtrainBuff = 0\n",
        "TestBuff = train_x.shape[1] - loadtrainBuff    # Testing period\n",
        "if testTrainBuff is True:\n",
        "  xTestBuff = train_x[:, -TestBuff:, :]\n",
        "  val_x = np.concatenate([xTestBuff, val_x], axis=1)\n",
        "model.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBOVdmuGY-Hi"
      },
      "source": [
        "### Define Testing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJjYMsPTWGX5"
      },
      "outputs": [],
      "source": [
        "from hydroDL.model.settings import make_test_settings\n",
        "\n",
        "def specify_inputs(inputs, settings, i):\n",
        "    \"\"\"\n",
        "    Pulls x and c data based on i and the settings\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    _inputs_ = inputs\n",
        "    _inputs_[\"xTemp\"] = inputs[\"x\"][settings[\"iS\"][i] : settings[\"iE\"][i], :, :]\n",
        "    if _inputs_[\"c\"] is not None:\n",
        "        _inputs_[\"cTemp\"] = np.repeat(\n",
        "            np.reshape(\n",
        "                inputs[\"c\"][settings[\"iS\"][i] : settings[\"iE\"][i], :],\n",
        "                [settings[\"iE\"][i] - settings[\"iS\"][i], 1, settings[\"nc\"]],\n",
        "            ),\n",
        "            settings[\"nt\"],\n",
        "            axis=1,\n",
        "        )\n",
        "        _inputs_[\"xTest\"] = torch.from_numpy(\n",
        "            np.swapaxes(np.concatenate([_inputs_[\"xTemp\"], _inputs_[\"cTemp\"]], 2), 1, 0)\n",
        "        ).float()\n",
        "    else:\n",
        "        _inputs_[\"xTest\"] = torch.from_numpy(\n",
        "            np.swapaxes(_inputs_[\"xTemp\"], 1, 0)\n",
        "        ).float()\n",
        "    if torch.cuda.is_available():\n",
        "        _inputs_[\"xTest\"] = _inputs_[\"xTest\"].cuda()\n",
        "    return _inputs_\n",
        "\n",
        "from hydroDL.model.settings import make_test_settings\n",
        "def testModel(\n",
        "    model,\n",
        "    x,\n",
        "    c,\n",
        "    *,\n",
        "    batchSize=None,\n",
        "    filePathLst=None,\n",
        "    doMC=False,\n",
        "    outModel=None,\n",
        "    savePath=None,\n",
        "):\n",
        "\n",
        "    fLst = list()\n",
        "    for filePath in filePathLst:\n",
        "        if os.path.exists(filePath):\n",
        "            os.remove(filePath)\n",
        "        f = open(filePath, \"a\")\n",
        "        fLst.append(f)\n",
        "    inputs, settings = make_test_settings(model, x, c, batchSize, doMC, outModel)\n",
        "    model.train(mode=False)\n",
        "    for i in range(len(settings[\"iS\"])):\n",
        "        log.debug(f\"batch {i}\")\n",
        "        data = specify_inputs(inputs, settings, i)\n",
        "        print(f'iter {i}, working on test data dimension: ',data[\"xTest\"].shape)\n",
        "        yP = model(data[\"xTest\"])\n",
        "\n",
        "        # CP-- marks the beginning of problematic merge\n",
        "        yOut = yP.detach().cpu().numpy().swapaxes(0, 1)\n",
        "\n",
        "        # save output\n",
        "        for k in range(settings[\"ny\"]):\n",
        "            f = fLst[k]\n",
        "            pd.DataFrame(yOut[:, :, k]).to_csv(f, header=False, index=False)\n",
        "        model.zero_grad()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    for f in fLst:\n",
        "        f.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enY3AHIzZEbz"
      },
      "source": [
        "### Model Testing and Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7RaQABaiERY",
        "outputId": "68017f77-8464-4614-841d-aac61b4076bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n",
            "read master file /content/output/rnnStreamflow/CAMELSDemo/TestRun/epochs20_batch100_rho365_hiddensize256_Tstart19801001_Tend19951001_trainBuff365_flowregime1/All/master.json\n",
            "iter 0, working on test data dimension:  torch.Size([10957, 50, 40])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/hydroDLpack/hydroDL/model/rnn/CudnnLstm.py:105: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)\n",
            "  output, hy, cy, reserve, new_weight_buf = torch._cudnn_rnn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 1, working on test data dimension:  torch.Size([10957, 50, 40])\n",
            "iter 2, working on test data dimension:  torch.Size([10957, 50, 40])\n",
            "iter 3, working on test data dimension:  torch.Size([10957, 50, 40])\n",
            "iter 4, working on test data dimension:  torch.Size([10957, 50, 40])\n",
            "iter 5, working on test data dimension:  torch.Size([10957, 50, 40])\n",
            "iter 6, working on test data dimension:  torch.Size([10957, 50, 40])\n",
            "iter 7, working on test data dimension:  torch.Size([10957, 50, 40])\n",
            "iter 8, working on test data dimension:  torch.Size([10957, 50, 40])\n",
            "iter 9, working on test data dimension:  torch.Size([10957, 50, 40])\n",
            "iter 10, working on test data dimension:  torch.Size([10957, 50, 40])\n",
            "iter 11, working on test data dimension:  torch.Size([10957, 50, 40])\n",
            "iter 12, working on test data dimension:  torch.Size([10957, 50, 40])\n",
            "iter 13, working on test data dimension:  torch.Size([10957, 21, 40])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-5bcc6b03119f>:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  pred = pd.read_csv(filePathLst[0], dtype=np.float, header=None).values[:, :, None]\n"
          ]
        }
      ],
      "source": [
        "from hydroDL import master, utils\n",
        "from hydroDL.post import stat\n",
        "print(valid_batch)\n",
        "# load and forward the model for validation\n",
        "test_model = loadModel(output_path, epoch=valid_epoch)\n",
        "filePathLst = master.master.namePred(\n",
        "    output_path, valid_date, \"All\", epoch=valid_epoch\n",
        ")  # prepare the name of csv files to save testing results\n",
        "testModel(test_model, val_x, c=val_c, batchSize=valid_batch, filePathLst=filePathLst)\n",
        "\n",
        "pred = pd.read_csv(filePathLst[0], dtype=np.float, header=None).values[:, :, None]\n",
        "# transform back to the original observation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "temppred = transNormbyDic(\n",
        "      pred, \"runoff\", scaler.stat_dict, log_norm_cols = log_norm_cols, to_norm=False)\n",
        "\n",
        "pred = basinNorm(temppred,  basinarea, meanprep, toNorm=False)\n",
        "\n",
        "pred = pred[:,TestBuff:,:]* 0.0283168\n",
        "\n",
        "obs = val_y* 0.0283168\n",
        "# calculate statistic metrics\n",
        "statDictLst = stat.statError(pred.squeeze(), obs.squeeze())\n",
        "print(np.nanmedian(statDictLst[\"NSE\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXADqC7ppgvP",
        "outputId": "2b4f56c5-b436-4f8f-c554-ad4c8daebd73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.722787794841875\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}